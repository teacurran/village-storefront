# Kubernetes Deployment for Village Storefront

## Overview

This directory contains Kubernetes manifests for deploying the Village Storefront background job workers with priority queuing, retry policies, dead-letter handling, and horizontal pod autoscaling.

**Related Documentation:**
- Operations Runbook: `docs/operations/job_runbook.md`
- Architecture: `docs/architecture/04_Operational_Architecture.md` (Section 3.6)
- Task: I3.T6 - Background Job Framework Enhancements

---

## Directory Structure

```
k8s/
├── base/
│   ├── deployment-workers.yaml           # General-purpose worker deployment
│   ├── deployment-workers-critical.yaml  # Dedicated CRITICAL/HIGH priority workers
│   └── kustomization.yaml                # Kustomize base configuration
└── README.md                             # This file
```

---

## Deployment Architecture

### Worker Pools

1. **General-Purpose Workers** (`deployment-workers.yaml`)
   - Processes all priority levels (CRITICAL → BULK)
   - Default: 3 replicas, scales 2-20 based on CPU/memory
   - Resource requests: 250m CPU, 512Mi memory
   - Use case: MVP deployment, cost-effective for mixed workloads

2. **Critical Workers** (`deployment-workers-critical.yaml`)
   - Processes only CRITICAL and HIGH priority jobs
   - Default: 5 replicas, scales 3-30 based on queue depth
   - Resource requests: 500m CPU, 1Gi memory
   - Use case: Production deployment to prevent priority starvation

### Deployment Strategies

**Option A: Single Worker Pool (MVP)**
- Deploy only `deployment-workers.yaml`
- Simpler operations, lower resource usage
- Risk: Low-priority jobs can starve high-priority jobs during high load

**Option B: Dedicated Priority Pools (Production)**
- Deploy both `deployment-workers.yaml` and `deployment-workers-critical.yaml`
- CRITICAL workers handle urgent jobs (payments, notifications)
- General workers handle reporting, analytics, bulk operations
- Higher resource usage but guaranteed SLA compliance

---

## Prerequisites

### 1. Namespace

```bash
kubectl create namespace village-storefront
```

### 2. Service Account

```bash
kubectl create serviceaccount village-storefront -n village-storefront
```

### 3. Secrets

Create required secrets for database, object storage, and external services:

```bash
# Database credentials
kubectl create secret generic village-storefront-db \
  --namespace=village-storefront \
  --from-literal=jdbc-url='jdbc:postgresql://postgres.default.svc.cluster.local:5432/storefront' \
  --from-literal=username='storefront' \
  --from-literal=password='<DB_PASSWORD>'

# Cloudflare R2 credentials
kubectl create secret generic village-storefront-r2 \
  --namespace=village-storefront \
  --from-literal=access-key-id='<R2_ACCESS_KEY>' \
  --from-literal=secret-access-key='<R2_SECRET_KEY>'

# Stripe API key
kubectl create secret generic village-storefront-stripe \
  --namespace=village-storefront \
  --from-literal=api-key='sk_live_...'
```

**Production:** Use [Sealed Secrets](https://github.com/bitnami-labs/sealed-secrets) or [External Secrets Operator](https://external-secrets.io/) to manage secrets securely.

### 4. ConfigMap

ConfigMap is auto-generated by Kustomize. To customize:

```bash
kubectl edit configmap village-storefront-config -n village-storefront
```

---

## Deployment

### Using Kustomize (Recommended)

```bash
# Deploy base configuration (general-purpose workers)
kubectl apply -k k8s/base/

# Verify deployment
kubectl get deployments -n village-storefront
kubectl get pods -n village-storefront
kubectl get hpa -n village-storefront
```

### Using kubectl (Direct)

```bash
# Deploy general-purpose workers
kubectl apply -f k8s/base/deployment-workers.yaml -n village-storefront

# Optionally deploy critical workers
kubectl apply -f k8s/base/deployment-workers-critical.yaml -n village-storefront
```

### Verify Health

```bash
# Check pod status
kubectl get pods -l app=village-storefront -n village-storefront

# Check pod logs
kubectl logs -l component=workers -n village-storefront --tail=50

# Check health endpoint
kubectl port-forward -n village-storefront deployment/village-storefront-workers 8080:8080
curl http://localhost:8080/q/health

# Check Prometheus metrics
curl http://localhost:8080/q/metrics | grep queue
```

---

## Configuration

### Environment Variables

Key configuration options (set in deployment YAML or via ConfigMap):

| Variable | Default | Description |
|----------|---------|-------------|
| `QUARKUS_SCHEDULER_ENABLED` | `true` | Enable/disable background job processing |
| `JOBS_QUEUE_CAPACITY_CRITICAL` | `1000` | Max CRITICAL queue depth before overflow |
| `JOBS_QUEUE_CAPACITY_DEFAULT` | `10000` | Max DEFAULT queue depth |
| `JOBS_RETRY_MAX_ATTEMPTS_CRITICAL` | `5` | Max retry attempts for CRITICAL jobs |
| `JOBS_RETRY_MAX_ATTEMPTS_DEFAULT` | `3` | Max retry attempts for DEFAULT jobs |
| `WORKER_PRIORITY_FILTER` | (none) | Comma-separated priorities to process (e.g., `CRITICAL,HIGH`) |

### Resource Limits

**General Workers:**
```yaml
resources:
  requests:
    cpu: "250m"
    memory: "512Mi"
  limits:
    cpu: "1000m"
    memory: "2Gi"
```

**Critical Workers:**
```yaml
resources:
  requests:
    cpu: "500m"
    memory: "1Gi"
  limits:
    cpu: "2000m"
    memory: "4Gi"
```

Adjust based on workload profiling and cost constraints.

### Horizontal Pod Autoscaling

**General Workers HPA:**
- Min replicas: 2
- Max replicas: 20
- Scale-up trigger: CPU > 70%, Memory > 80%
- Scale-up rate: 50% increase every 60s
- Scale-down rate: 1 pod every 60s after 5min stabilization

**Critical Workers HPA:**
- Min replicas: 3
- Max replicas: 30
- Scale-up trigger: CPU > 60%
- Scale-up rate: 100% increase every 30s (aggressive)
- Scale-down rate: 1 pod every 120s after 10min stabilization

**Custom Metrics (Future):**

When Prometheus Adapter is configured, enable queue-depth-based scaling:

```yaml
metrics:
- type: Pods
  pods:
    metric:
      name: reporting_refresh_queue_depth
    target:
      type: AverageValue
      averageValue: "100"
```

---

## Monitoring

### Prometheus Metrics

Workers expose metrics at `/q/metrics` endpoint. Key metrics:

```promql
# Queue depth per priority
reporting_refresh_queue_depth{priority="critical"}
reporting_export_queue_depth{priority="default"}

# Job throughput
rate(reporting_job_completed[5m])
rate(reporting_job_failed[5m])

# Dead-letter queue depth
reporting_refresh_dlq_depth
reporting_export_dlq_depth

# Job latency (p95)
histogram_quantile(0.95, rate(reporting_job_duration_bucket[5m]))
```

### Grafana Dashboards

Import dashboard: `Background Job Health` (ID: TBD)

Panels:
1. Queue depth by priority (stacked area)
2. Job completion rate vs. failure rate
3. DLQ depth over time
4. Job duration p50/p95/p99
5. Worker pod CPU/memory usage
6. HPA scaling events

### Alerts

Recommended alert rules (Prometheus):

```yaml
groups:
- name: background_jobs
  interval: 30s
  rules:
  - alert: CriticalQueueBacklog
    expr: reporting_refresh_queue_depth{priority="critical"} > 100
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "CRITICAL job queue backing up"
      description: "CRITICAL queue depth {{ $value }} exceeds threshold for 2 minutes"

  - alert: DeadLetterQueueGrowing
    expr: rate(reporting_refresh_dlq_depth[5m]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Dead-letter queue accumulating jobs"
      description: "DLQ growing - investigate failed jobs"

  - alert: JobFailureRateHigh
    expr: rate(reporting_job_failed[5m]) / rate(reporting_job_started[5m]) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Job failure rate exceeds 10%"
      description: "{{ $value | humanizePercentage }} of jobs failing"
```

---

## Operations

### Scaling Workers Manually

```bash
# Scale general workers
kubectl scale deployment/village-storefront-workers --replicas=10 -n village-storefront

# Scale critical workers
kubectl scale deployment/village-storefront-workers-critical --replicas=15 -n village-storefront
```

### Pausing Job Processing

```bash
# Option 1: Scale to 0 replicas
kubectl scale deployment/village-storefront-workers --replicas=0 -n village-storefront

# Option 2: Disable scheduler via ConfigMap
kubectl set env deployment/village-storefront-workers \
  QUARKUS_SCHEDULER_ENABLED=false -n village-storefront
```

### Resuming Job Processing

```bash
# Restore replicas
kubectl scale deployment/village-storefront-workers --replicas=3 -n village-storefront

# Re-enable scheduler
kubectl set env deployment/village-storefront-workers \
  QUARKUS_SCHEDULER_ENABLED=true -n village-storefront
```

### Viewing Logs

```bash
# All worker logs
kubectl logs -l component=workers -n village-storefront --tail=100 -f

# Specific pod logs
kubectl logs village-storefront-workers-<pod-id> -n village-storefront

# Filter for job failures
kubectl logs -l component=workers -n village-storefront | grep "Job failed"
```

### Debugging

```bash
# Shell into worker pod
kubectl exec -it deployment/village-storefront-workers -n village-storefront -- /bin/sh

# Check health
curl http://localhost:8080/q/health

# View metrics
curl http://localhost:8080/q/metrics | grep queue

# Thread dump
jstack 1
```

---

## Rollout Strategy

### Rolling Update (Default)

```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 0
```

- New pods start before old pods terminate
- Zero downtime for job processing
- Gradual queue handoff

### Blue-Green Deployment

For major framework changes:

```bash
# Deploy new version as separate deployment
kubectl apply -f deployment-workers-v2.yaml -n village-storefront

# Monitor new version processing jobs
kubectl logs -l version=v2 -n village-storefront

# Switch traffic by scaling old version to 0
kubectl scale deployment/village-storefront-workers --replicas=0 -n village-storefront
kubectl scale deployment/village-storefront-workers-v2 --replicas=5 -n village-storefront
```

---

## Troubleshooting

### Pods Not Starting

**Check events:**
```bash
kubectl describe pod <pod-name> -n village-storefront
```

**Common issues:**
- Missing secrets: Verify `village-storefront-db`, `village-storefront-r2`, `village-storefront-stripe` exist
- Image pull errors: Check `imagePullPolicy` and registry credentials
- Resource constraints: Verify node capacity with `kubectl describe node`

### Jobs Not Processing

**Check scheduler status:**
```bash
kubectl exec deployment/village-storefront-workers -n village-storefront -- \
  curl -s http://localhost:8080/q/health | jq '.checks[] | select(.name | contains("scheduler"))'
```

**Check environment variables:**
```bash
kubectl exec deployment/village-storefront-workers -n village-storefront -- env | grep SCHEDULER
```

**Check logs for errors:**
```bash
kubectl logs -l component=workers -n village-storefront | grep -E "(ERROR|WARN)"
```

### High Memory Usage

**Check current usage:**
```bash
kubectl top pods -l component=workers -n village-storefront
```

**Increase memory limits:**
```bash
kubectl set resources deployment/village-storefront-workers \
  --limits=memory=4Gi -n village-storefront
```

**Heap dump for analysis:**
```bash
kubectl exec deployment/village-storefront-workers -n village-storefront -- \
  jcmd 1 GC.heap_dump /tmp/heap.hprof
```

---

## Migration Guide

### From Existing Job System

1. **Deploy framework in shadow mode:**
   - Keep existing job system running
   - Deploy new workers with `WORKER_PRIORITY_FILTER=BULK`
   - Monitor metrics for 7 days

2. **Gradual traffic shift:**
   - Move non-critical jobs (BULK, LOW) to new framework
   - Monitor DLQ and failure rates
   - Iterate on retry policies

3. **Full cutover:**
   - Migrate all jobs to new framework
   - Decommission old job system
   - Update runbooks and dashboards

---

## Related Documentation

- **Operations Runbook:** `docs/operations/job_runbook.md`
- **Architecture:** `docs/architecture/04_Operational_Architecture.md`
- **Integration Tests:** `src/test/java/villagecompute/storefront/services/jobs/JobSchedulerTest.java`
- **Framework Code:** `src/main/java/villagecompute/storefront/services/jobs/config/`

---

## Support

For operational issues:
- **On-call:** PagerDuty rotation
- **Slack:** `#incidents-storefront`
- **Status Page:** https://status.villagecompute.com
